import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score

train_x = np.load('SNU_Traindata_x_240823_1st.npy')
train_y = np.load('SNU_Traindata_y_240826_2nd_test.npy')

kf = KFold(n_splits=3, shuffle=True, random_state=42)
mse_scores = []
r2_scores = []
adj_r2_scores = []

for fold, (train_idx, test_idx) in enumerate(kf.split(sort_train_X)):
    print(f'Fold {fold + 1}')
    
    X_train, X_test = train_x[train_idx], train_x[test_idx]
    y_train, y_test = train_y[train_idx], train_y[test_idx]

    model = DecisionTreeRegressor(random_state=42, max_depth=3)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    mse_scores.append(mse)

    r2 = r2_score(y_test, y_pred)
    r2_scores.append(r2)

    n = len(y_test)
    p = X_train.shape[1] 
    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    adj_r2_scores.append(adj_r2)

    print(f'Fold {fold + 1} MSE: {mse:.4f}, R²: {r2:.4f}, Adjusted R²: {adj_r2:.4f}')

mean_mse = np.mean(mse_scores)
mean_r2 = np.mean(r2_scores)
mean_adj_r2 = np.mean(adj_r2_scores)

print(f'\nAverage MSE over {kf.n_splits} folds: {mean_mse:.4f}')
print(f'Average R² over {kf.n_splits} folds: {mean_r2:.4f}')
print(f'Average Adjusted R² over {kf.n_splits} folds: {mean_adj_r2:.4f}')
